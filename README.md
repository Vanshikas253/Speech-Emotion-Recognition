# Speech-Emotion-Recognition
Using Multi-Layer Perceptron Classifier and Extra Tree Classifier to predict the emotion of the speaker by analyzing the recorded audio clip of the speaker’s voice.

## Brief Description
This project was a part of my Foundations of Machine Learning Course at CentraleSupélec - Université Paris-Saclay. Here are the links for the datasets that we used for this project:  
•	The Ryerson Audio-Visual Database of Emotional Speech and Song(RAVDESS) - https://zenodo.org/record/1188976#.YX0Ldp5BxZW  
•	Crowd-Sourced Emotional Multimodal Actors Dataset (Crema-D) - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/   
•	Surrey Audio-Visual Expressed Emotion (SAVEE) Database- http://kahlan.eps.surrey.ac.uk/savee/  
•	Toronto emotional speech set (TESS)- https://tspace.library.utoronto.ca/handle/1807/24487  

 
